{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd246f39",
   "metadata": {},
   "source": [
    "# Implementing Nerual Networks Model to classify the given image into 1 of the 10 categories of the fashion MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9862a1a8",
   "metadata": {},
   "source": [
    "## Step 1: Importing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fd8ba20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\NumberMNIST\n"
     ]
    }
   ],
   "source": [
    "path = r\"C:\\NumberMNIST\"\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b3feaf",
   "metadata": {},
   "source": [
    "## Step 2: Importing necessary libraries and loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0de8e58e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t10k-images-idx3-ubyte',\n",
       " 't10k-images.idx3-ubyte',\n",
       " 't10k-labels-idx1-ubyte',\n",
       " 't10k-labels.idx1-ubyte',\n",
       " 'train-images-idx3-ubyte',\n",
       " 'train-images.idx3-ubyte',\n",
       " 'train-labels-idx1-ubyte',\n",
       " 'train-labels.idx1-ubyte']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import os\n",
    "\n",
    "def load_images(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        magic, num_images, rows, cols = struct.unpack('>IIII', f.read(16))\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8).reshape(num_images, rows * cols)\n",
    "    return images\n",
    "\n",
    "def load_labels(file_path):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        magic, num_labels = struct.unpack('>II', f.read(8))\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n",
    "os.listdir(path) # List files in the dataset directory to verify paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a994a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (60000, 784)\n",
      "Test shape: (10000, 784)\n",
      "Train labels shape: (60000,)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "X_train = load_images(path + r\"\\train-images.idx3-ubyte\")\n",
    "y_train = load_labels(path + r\"\\train-labels.idx1-ubyte\")\n",
    "\n",
    "X_test = load_images(path + r\"\\t10k-images.idx3-ubyte\")\n",
    "y_test = load_labels(path + r\"\\t10k-labels.idx1-ubyte\")\n",
    "\n",
    "print(\"Train shape:\", X_train.shape)\n",
    "print(\"Test shape:\", X_test.shape)\n",
    "print(\"Train labels shape:\", y_train.shape)\n",
    "print(\"Test labels shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff069ab",
   "metadata": {},
   "source": [
    "## Step 3: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "928ee995",
   "metadata": {},
   "source": [
    "**Flattening** is the process of converting a multi-dimensional input (such as a 2D image matrix) into a one-dimensional vector so that it can be fed into a fully connected neural network.\n",
    "\n",
    "Here, flattening is the process of converting each 28Ã—28 image into a 784-dimensional vector so that it can be used as input to a fully connected neural network.\n",
    "\n",
    "**In this case, flattening images is not necessary as they are already in the shape (num_samples, 784)!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb32560c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 255)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(X_train), np.max(X_train) # Check pixel value range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de0947ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize pixel values to [0, 1]\n",
    "X_train= X_train / 255.0\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "# Why 255.0? Because pixel values in the dataset are in the range [0, 255], \n",
    "# and dividing by 255.0 scales them to the range [0, 1], \n",
    "# which is beneficial for training neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecb6b461",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels in y_train: [0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique labels in y_train: {np.unique(y_train)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45c73f4",
   "metadata": {},
   "source": [
    "Neural network needs 0s and 1s, not other numbers, hence we one-hot encode the original labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d2c0536e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoded train labels shape: (60000, 10)\n",
      "One-hot encoded test labels shape: (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "# One-hot encode labels\n",
    "\n",
    "def one_hot_encode(y, num_classes):\n",
    "    one_hot_matrix= np.zeros((y.shape[0], num_classes))\n",
    "    for i in range(y.shape[0]):\n",
    "        one_hot_matrix[i, y[i]] = 1\n",
    "    return one_hot_matrix\n",
    "\n",
    "y_train_encoded = one_hot_encode(y_train, 10)\n",
    "y_test_encoded = one_hot_encode(y_test, 10)\n",
    "\n",
    "print(\"One-hot encoded train labels shape:\", y_train_encoded.shape)\n",
    "print(\"One-hot encoded test labels shape:\", y_test_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e36df6",
   "metadata": {},
   "source": [
    "**What One-Hot Encoding Does:**\n",
    "\n",
    "If:\n",
    "\n",
    "y = [2, 0, 3]\n",
    "num_classes = 4\n",
    "\n",
    "\n",
    "Then output becomes:\n",
    "\n",
    "[[0 0 1 0]\n",
    "[1 0 0 0]\n",
    "[0 0 0 1]]\n",
    "\n",
    "where index of each one in each 1-D array denotes the class (for eg., index 2 -> Class 2).\n",
    "\n",
    "**-> One-hot encoding is required for computing cross-entropy loss in multi-class classification problems.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cca5efc",
   "metadata": {},
   "source": [
    "Now initially, the shape of the labels were:\n",
    "\n",
    "Train labels shape: (60000,)\n",
    "Test labels shape: (10000,)\n",
    "\n",
    "i.e., they were 1D array initally.\n",
    "\n",
    "After one-hot encoding:\n",
    "\n",
    "One-hot encoded train labels shape: (60000, 10)\n",
    "One-hot encoded test labels shape: (10000, 10)\n",
    "\n",
    "\n",
    "i.e., they are 2D matrices now.\n",
    "\n",
    "Now the question is: **Why Do We Need 2D Matrices?**\n",
    "\n",
    "It's because we are training on many samples at once, not one image at a time.\n",
    "\n",
    "Neural networks use matrix multiplication to process batches efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d44203",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class 0: 5923 samples\n",
      "Class 1: 6742 samples\n",
      "Class 2: 5958 samples\n",
      "Class 3: 6131 samples\n",
      "Class 4: 5842 samples\n",
      "Class 5: 5421 samples\n",
      "Class 6: 5918 samples\n",
      "Class 7: 6265 samples\n",
      "Class 8: 5851 samples\n",
      "Class 9: 5949 samples\n"
     ]
    }
   ],
   "source": [
    "# Quick dataset analysis\n",
    "\n",
    "unique, counts = np.unique(y_train, return_counts=True)\n",
    "\n",
    "for u, c in zip(unique, counts):\n",
    "    print(f\"Class {u}: {c} samples\")\n",
    "\n",
    "# zip() is a built-in Python function that lets you iterate over \n",
    "# multiple sequences (or lists) at the same time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d50a8af",
   "metadata": {},
   "source": [
    "Here, each class has around 6000 samples, hence the dataset is **mostly balanced**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef085a",
   "metadata": {},
   "source": [
    "Before we move on to the next step, we have three options for training: \n",
    "\n",
    "1. Batch Gradient Descent (use all 60,000 samples at once)\n",
    "2. Mini-Batch Gradient Descent (use small batches like 32/64/128)\n",
    "3. Stochastic Gradient Descent (1 sample at a time)\n",
    "\n",
    "We will pick the second one here because it has faster convergence, more stable than SGD and has better generalisation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20b69d6",
   "metadata": {},
   "source": [
    "### Step 4: Implementing Mini-Batch Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba72a7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prevents learning bias by ensuring that the model sees a \n",
    "# balanced number of samples from each class during training.\n",
    "\n",
    "# aragne(): Creates an array of indices from \n",
    "# 0 to the number of samples in X.\n",
    "# Not to be confused with range(), which creates a list of integers.\n",
    "\n",
    "def shuffle_data(X, Y):\n",
    "    indices= np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices) # Shuffle the indices in place\n",
    "\n",
    "    return X[indices], Y[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31b739dd",
   "metadata": {},
   "source": [
    "**NOTE:** What's reproducibility? It means that if you run the code multiple times, you will get the same results each time. This is important for debugging and comparing results.\n",
    "\n",
    "This is achieved by np.random.seed()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d49ddbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Softmax function\n",
    "def softmax(z):\n",
    "\n",
    "    #keepdims=True keeps the dimensions of the output the same as the input,\n",
    "    # which is important for broadcasting during division.\n",
    "\n",
    "    # How? By subtracting the maximum value in each row of z from every element in that row,\n",
    "    # we ensure that the largest value in each row becomes 0, and all other values become negative.\n",
    "\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True)) # For numerical stability\n",
    "    \n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1f011fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relu activation function\n",
    "\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b7dd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward propagation function\n",
    "\n",
    "def forward_propagation(X, W1, b1, W2, b2):\n",
    "    \n",
    "    # Here, z1 represents the pre-activation output of the hidden layer,\n",
    "    # which is calculated by performing a linear transformation \n",
    "    # on the input batch (X_batch)\n",
    "\n",
    "    z1= np.dot(X, W1) + b1 # Linear transformation for hidden layer\n",
    "            \n",
    "    a1= relu(z1) # Calling relu() for hidden layer\n",
    "\n",
    "    # z2 represents the pre-activation output of the output layer, \n",
    "    # which is calculated by performing a linear transformation on \n",
    "    # the activated hidden layer(a1)\n",
    "\n",
    "    z2= np.dot(a1, W2) + b2 # Linear transformation for output layer\n",
    "\n",
    "    # Calling softmax on z2 converts the raw output scores into probabilities\n",
    "    # that sum to 1 across the output classes.\n",
    "\n",
    "    a2= softmax(z2) # Softmax activation for output layer\n",
    "\n",
    "    return a1, a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d32850b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining categorical cross-entropy loss function\n",
    "\n",
    "def cat_cross_entropy_loss(m, y_batch, a2):\n",
    "    # m is the number of samples in the batch\n",
    "    # y_batch is the true one-hot encoded labels for the batch\n",
    "    # a2 is the predicted probabilities from the output layer\n",
    "\n",
    "    # Adding a small epsilon to prevent log(0)\n",
    "    epsilon = 1e-8\n",
    "\n",
    "    # Calculate the loss using the categorical cross-entropy formula\n",
    "    loss = -np.sum(y_batch * np.log(a2 + epsilon)) / m\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a241422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining backward propagation function\n",
    "\n",
    "def back_prop(a1, a2, X_batch, y_batch, m, w2, z1):\n",
    "\n",
    "    dZ2= a2 - y_batch # Gradient of loss wrt z2 (output layer pre-activation)\n",
    "    \n",
    "    dW2= np.dot(a1.T, dZ2) / m # Gradient of loss wrt to W2\n",
    "    db2= np.sum(dZ2, axis=0, keepdims=True) / m # Gradient of loss wrt b2\n",
    "\n",
    "    dA1= np.dot(dZ2, w2.T) # Gradient of loss wrt a1 (hidden layer activation)\n",
    "    dZ1= dA1 * (z1 > 0)     # Gradient of loss wrt z1 (hidden layer pre-activation), applying ReLU derivative\n",
    "\n",
    "    dW1= np.dot(X_batch.T, dZ1) / m # Gradient of loss wrt W1\n",
    "    db1= np.sum(dZ1, axis= 0, keepdims= True) / m # Gradient of loss wrt b1\n",
    "\n",
    "    return dW1, db1, dW2, db2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b885f50",
   "metadata": {},
   "source": [
    "**NOTE:** The activated output of the hidden layer represents learned feature transformations, whereas the output layer produces class probabilities via the softmax function, which are used for final classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d4b9286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "\n",
    "def train(batch_size, epochs, lr):\n",
    "\n",
    "    input_size= X_train.shape[1] # 784 for 28x28 images\n",
    "    output_size= y_train_encoded.shape[1] # 10 for 10 classes (0 - 9)\n",
    "\n",
    "    # He initialisation for weights (important for ReLU activation)\n",
    "    np.random.seed(100) # Ensure reproducibility\n",
    "\n",
    "    W1= np.random.randn(input_size, 100) * np.sqrt(2 / input_size) # 100 neurons in hidden layer\n",
    "    b1= np.zeros((1, 100)) # Bias for hidden layer\n",
    "\n",
    "    W2= np.random.randn(100, output_size) * np.sqrt(2 / 100) # Output layer weights\n",
    "    b2= np.zeros((1, output_size)) # Bias for output layer\n",
    "\n",
    "    history= [] # To store loss history\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Shuffle training data at the beginning of each epoch\n",
    "        indices= np.random.permutation(X_train.shape[0]) # Shuffled indices for the entire dataset\n",
    "\n",
    "        X_train_shuffled= X_train[indices]\n",
    "        y_train_shuffled= y_train_encoded[indices]\n",
    "\n",
    "        epoch_loss= 0\n",
    "\n",
    "        for i in range(0, X_train.shape[0], batch_size):\n",
    "\n",
    "            X_batch= X_train_shuffled[i: i + batch_size]\n",
    "            y_batch= y_train_shuffled[i: i+ batch_size]\n",
    "\n",
    "            # Calling forward_propagation() to get activations\n",
    "            a1, a2= forward_propagation(X_batch, W1, b1, W2, b2)\n",
    "\n",
    "            # Here, \n",
    "            \n",
    "            # a1: The activated output of the hidden layer, \n",
    "            # which is used in backpropagation to compute gradients for the \n",
    "            # weights and biases of both layers.\n",
    "\n",
    "            # a2: The predicted probabilities from the output layer, \n",
    "            # which is used to calculate gradients in backpropagation.\n",
    "\n",
    "            # Calculate loss for the current batch\n",
    "            epoch_loss += cat_cross_entropy_loss(X_batch.shape[0], y_batch, a2)\n",
    "\n",
    "            # Now, forward propagation computes predictions and computes loss\n",
    "            # but it DOES NOT UPDATE WEIGHTS! It only answers: \n",
    "            # \"GIVEN current weights, what is the prediction?\"\n",
    "\n",
    "            # Hence we need back prop as well\n",
    "\n",
    "            # Calling back_prop() to compute gradients\n",
    "            dW1, db1, dW2, db2= back_prop(a1, a2, X_batch, y_batch, X_batch.shape[0], W2, np.dot(X_batch, W1) + b1)\n",
    "\n",
    "            # Update weights and biases using gradient descent\n",
    "            W1 -= lr * dW1\n",
    "            b1 -= lr * db1\n",
    "            W2 -= lr * dW2\n",
    "            b2 -= lr * db2\n",
    "\n",
    "        history.append(epoch_loss / (X_train.shape[0] / batch_size)) # Average loss per epoch\n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {history[-1]:.4f}\")\n",
    "\n",
    "    return W1, b1, W2, b2, history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1f3965",
   "metadata": {},
   "source": [
    "### Step 5: Predicting and Evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5a97e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
